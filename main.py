import os
import requests
import json
from google import genai
from newspaper import Article
from datetime import datetime, timedelta

# í™˜ê²½ ì„¤ì •
NAVER_ID = os.environ['NAVER_CLIENT_ID']
NAVER_SECRET = os.environ['NAVER_CLIENT_SECRET']
client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])
MODEL_NAME = 'gemini-2.5-flash'

def get_24h_news():
    # [1ë‹¨ê³„] 24ì‹œê°„ ì´ë‚´ì˜ AI ê´€ë ¨ ê¸°ì‚¬ íƒìƒ‰
    query = "AI OR ai OR ì¸ê³µì§€ëŠ¥"
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=100&sort=date"
    headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
    res = requests.get(url, headers=headers).json().get('items', [])
    
    # ì‹œê°„ í•„í„°ë§ (í˜„ì¬ ì‹œê°„ ê¸°ì¤€ 24ì‹œê°„ ì „ê¹Œì§€)
    filtered_news = []
    now = datetime.now()
    for item in res:
        # ë„¤ì´ë²„ ë‚ ì§œ í˜•ì‹: "Tue, 04 Feb 2026 10:00:00 +0900"
        pub_date = datetime.strptime(item['pubDate'][:-6], "%a, %d %b %Y %H:%M:%S")
        if now - pub_date <= timedelta(hours=24):
            filtered_news.append(item)
    return filtered_news

def analyze_and_publish():
    news_pool = get_24h_news()
    
    # [2~3ë‹¨ê³„] ì œëª© ê¸°ì¤€ ë¶„ë¥˜ ë° ì¤‘ìš”ë„(ì¤‘ë³µë„) íŒë‹¨
    initial_analysis_prompt = f"""
    ë‹¹ì‹ ì€ aiì„œë¹„ìŠ¤ ë¸Œëœë“œ ê¸°íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ë“¤ì„ ë¶„ì„í•˜ì—¬ ê° ì¹´í…Œê³ ë¦¬(ê²½ì œ, ì‚¬íšŒ, ìƒí™œ&ë¬¸í™”, ì‚°ì—…, ì •ì¹˜, it&ê³¼í•™, í•´ì™¸)ë³„ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    ê°™ì€ ì£¼ì œì˜ ê¸°ì‚¬ê°€ ë§ì„ìˆ˜ë¡ í•´ë‹¹ ì£¼ì œë¥¼ ì¤‘ìš” ê¸°ì‚¬ë¡œ ì„ ë³„í•˜ì„¸ìš”.
    ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ì„ í›„ë³´ ê¸°ì‚¬(ë§í¬) 10ê°œë¥¼ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
    
    ë°ì´í„°: {news_pool}
    ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”: {{"ì¹´í…Œê³ ë¦¬ëª…": ["ë§í¬1", "ë§í¬2", ...]}}
    """
    
    analysis_res = client.models.generate_content(model=MODEL_NAME, contents=initial_analysis_prompt)
    try:
        # JSON ì •ì œ ë£¨í‹´
        content = analysis_res.text
        target_map = json.loads(content[content.find('{'):content.rfind('}')+1])
    except:
        return print("AI ë¶„ë¥˜ ë‹¨ê³„ ì˜¤ë¥˜")

    final_sections = ""
    
    # [4~5ë‹¨ê³„] ë³¸ë¬¸ ë¶„ì„ ë° ì¤‘ë³µ ë°°ì œ (ëŒ€ì²´ ë¡œì§)
    for category, links in target_map.items():
        unique_articles = []
        seen_contents = ""
        
        for link in links:
            if len(unique_articles) >= 5: break
            try:
                article = Article(link, language='ko')
                article.download()
                article.parse()
                text = article.text[:1500]
                
                # ì¤‘ë³µ ë‚´ìš© ê²€ì¦ ë¡œì§
                check_prompt = f"ë‹¤ìŒ ê¸°ì‚¬ ë³¸ë¬¸ì´ ê¸°ì¡´ ê¸°ì‚¬ë“¤ê³¼ ë‚´ìš©ì´ 80% ì´ìƒ ê²¹ì¹˜ë‚˜ìš”? 'ë„¤' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µí•˜ì„¸ìš”.\nê¸°ì¡´: {seen_contents[:1000]}\nì‹ ê·œ: {text[:500]}"
                is_duplicate = client.models.generate_content(model=MODEL_NAME, contents=check_prompt).text
                
                if "ì•„ë‹ˆì˜¤" in is_duplicate:
                    unique_articles.append({"title": article.title, "text": text, "link": link})
                    seen_contents += " " + text
            except: continue
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìµœì¢… ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        if unique_articles:
            summary_prompt = f"ë‹¹ì‹ ì€ aiì„œë¹„ìŠ¤ ë¸Œëœë“œ ê¸°íš ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ë¸Œëœë“œ ì¸ì‚¬ì´íŠ¸ì™€ í•¨ê»˜ ìš”ì•½í•˜ì„¸ìš”: {unique_articles}"
            summary_html = client.models.generate_content(model=MODEL_NAME, contents=summary_prompt).text
            final_sections += f"<div>{summary_html}</div>"

    # HTML ì™„ì„±
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(f"<html><body style='font-family:sans-serif; padding:40px;'><h1>ğŸ¤– AI ë¸Œëœë“œ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸</h1>{final_sections}</body></html>")

if __name__ == "__main__":
    analyze_and_publish()
